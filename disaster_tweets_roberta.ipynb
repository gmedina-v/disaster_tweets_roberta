{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "disaster_tweets_roberta.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TiSovoMb4wCk"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNAgyGyGeKFlbw1uSMFpq7R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fbf1716e0a844c5798771e9c13dd5fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e21556df6128432cb8cccf77ffdce6c3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bb16b9884a6f468a8d48cc52a43d89d0",
              "IPY_MODEL_57a420cfc8af4156bcc123e45324c106"
            ]
          }
        },
        "e21556df6128432cb8cccf77ffdce6c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb16b9884a6f468a8d48cc52a43d89d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ad5df795f1ca4dcbb621ea44f955e599",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_abbba613a13c4e6aa6c94a3f9ab8864c"
          }
        },
        "57a420cfc8af4156bcc123e45324c106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7d959d96ef92485b9294eb8e7d37d84a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:29&lt;00:00, 30.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fff1565559e84e4ba5e42621a3389b5b"
          }
        },
        "ad5df795f1ca4dcbb621ea44f955e599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "abbba613a13c4e6aa6c94a3f9ab8864c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d959d96ef92485b9294eb8e7d37d84a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fff1565559e84e4ba5e42621a3389b5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7ac6bb0320a47afa4c32ed962fe1e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5098ebe9da6a4b60bfebec689880d6bf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cd1737c191fd4fd7bcf97db95dca2da8",
              "IPY_MODEL_a87d05db8d7d4b8da3d74a0c51b6b7bd"
            ]
          }
        },
        "5098ebe9da6a4b60bfebec689880d6bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd1737c191fd4fd7bcf97db95dca2da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b415473e25184809bdabbdf266f465b6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1175aeeef525490784ed1e79a973ed9c"
          }
        },
        "a87d05db8d7d4b8da3d74a0c51b6b7bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_52f6cd7563a945d39aa91171e2ece1af",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:01&lt;00:00, 279kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e2066e921c5b46b19939844213653e2a"
          }
        },
        "b415473e25184809bdabbdf266f465b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1175aeeef525490784ed1e79a973ed9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52f6cd7563a945d39aa91171e2ece1af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e2066e921c5b46b19939844213653e2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8dd4ccb945943b49c3f5aa25df65e2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_def658ed9aee4faba8b364307c2728c4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0469f4abdb304502850830690513ac14",
              "IPY_MODEL_532fc47b1b95448dbcfb1cfe1d59712d"
            ]
          }
        },
        "def658ed9aee4faba8b364307c2728c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0469f4abdb304502850830690513ac14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_000aaca73e0e4d34b93fb198bc7f1149",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d3624c491c14017941ef4a8a517a6c0"
          }
        },
        "532fc47b1b95448dbcfb1cfe1d59712d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1c9789f907554d29bb5a2378f4a8857b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 1.54MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2fe0c6a2ad5d4db090d6a8d9f152a202"
          }
        },
        "000aaca73e0e4d34b93fb198bc7f1149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d3624c491c14017941ef4a8a517a6c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c9789f907554d29bb5a2378f4a8857b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2fe0c6a2ad5d4db090d6a8d9f152a202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "221042f75b774ef7a96611759807ebce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fadf7ffe467d4d70962bbb5704420850",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_33cfe5c01d9e4cbcb0587c56828d6f5c",
              "IPY_MODEL_b7b3e6e8f160443cb120b3d15c7f42c6"
            ]
          }
        },
        "fadf7ffe467d4d70962bbb5704420850": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33cfe5c01d9e4cbcb0587c56828d6f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_18a0d494903a4bea8c9e0ecd033f8301",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 482,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 482,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1511354b81424002a7d6d6086208d991"
          }
        },
        "b7b3e6e8f160443cb120b3d15c7f42c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ecfa493d2e20446bb4b92d6ba6bef182",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 482/482 [00:26&lt;00:00, 17.9B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60a84a117ace40e997f1c28f517f7b9b"
          }
        },
        "18a0d494903a4bea8c9e0ecd033f8301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1511354b81424002a7d6d6086208d991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ecfa493d2e20446bb4b92d6ba6bef182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60a84a117ace40e997f1c28f517f7b9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83ac1a7119d5474189cda0c916d64c08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1d8c49dde70543eeb8e94db338addc8c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_33db587a832a4bbb9cfd5f7e9a5b9df4",
              "IPY_MODEL_24131f99d0a743cbb88a9a63c9ac81d2"
            ]
          }
        },
        "1d8c49dde70543eeb8e94db338addc8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33db587a832a4bbb9cfd5f7e9a5b9df4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_674b87057a6842a1b71bdc7902c26050",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1634375628,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1634375628,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2df63f92ebfd453ebfc97980ad59ab57"
          }
        },
        "24131f99d0a743cbb88a9a63c9ac81d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_22a26991a9a249029f2588120f36d5fe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.63G/1.63G [00:21&lt;00:00, 76.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c65719691c724fdd99694c33348cd345"
          }
        },
        "674b87057a6842a1b71bdc7902c26050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2df63f92ebfd453ebfc97980ad59ab57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22a26991a9a249029f2588120f36d5fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c65719691c724fdd99694c33348cd345": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmedina-v/disaster_tweets_roberta/blob/main/disaster_tweets_roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU0AiyCq4X6c"
      },
      "source": [
        "# Disaster tweets with RoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLZIjUu54iOH"
      },
      "source": [
        "\r\n",
        "\r\n",
        "In this notebook we tackle the [disaster tweets](https://www.kaggle.com/c/nlp-getting-started) Kaggle competition using ðŸ¤— Huggingface's transformers. Given a tweet, the task is to predict whether it is about a disaster or not. We will leverage an implementation of RoBERTa to solve this task, a language model based on the transformer architecture.\r\n",
        "\r\n",
        "The challenge is to predict if a tweet refers to an ocurring disaster or if it is about something else. Language is full of figuritave expressions, so it is not straightforward to come up with a rule to classify text as belonging to one or other category that works every time. Even for a human, a message can be difficult to interpret without the appropriate context, which can lead to, sometimes funny, misunderstandings.\r\n",
        "\r\n",
        "As this is a binary classification task, in principle, any type of classifier can be used, such as logistic regression, SVM, random forest and feed-forward neural networks. These methods make use of the bag of words (BOW) approach to create numerical features, where the order of the words in the text and ther relations are ignored. However, language is a sequential phenomenon and words in a sentence have complex relations between them. More sophisticated language models must be used to capture these relations and extract meaningful information from textual data. The present analysis makes use of RoBERTa, a type of transformer language model put forward by [Facebook AI](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/). RoBERTa is a version of BERT which has been trained on a larger corpus for a longer time to achieve better performance in NLU (natural language understanding) tasks. BERT, in turn, is a transformer model originally proposed by [Google Research](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html). A short technical introduction is in order now before we start the analysis.\r\n",
        "\r\n",
        "A [transformer](https://arxiv.org/abs/1706.03762) is a type of artificial neural network that consists of an encoder and a decoder, these are processing blocks that are composed of 'attention' layers. In this context, 'attention' can be thought of as a mechanism to relate inputs and outputs through time. The encoder constructs a high-dimensional numerical representation of textual data. In this form, documents are converted into numeric tensors. The decoder produces an output which depends on both the information from the encoder and on all the previous outputs of the decoder. An example of this type of architecture is [BERT](https://arxiv.org/abs/1810.04805v2) (Bidirectional Encoder Representations from Transformers), a highly-complex model composed of stacks of bi-directional transformers and trained on the BooksCorpus (800M words) and English Wikipedia (2,500M words). BERT can be used for many NLU (natural laguage understanding) tasks, including document classification. RoBERTa is a version of BERT that has been trained on a slightly modified task and with a larger corpus, including news articles, outperforming BERT on all GLUE tasks.\r\n",
        "\r\n",
        "A publicly available implementation of this model (and many more) is offered by a popular python library called [transformers](https://huggingface.co/transformers/) created by ðŸ¤— [Huggingface](https://huggingface.co/). The models are available both as PyTorch and Tensorflow models, and checkpoints are available which allow easy access to trained models ready for use. This notebook will show step by step how to use this library to solve a classification task with relevant explanations in place.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiSovoMb4wCk"
      },
      "source": [
        "## Contents\r\n",
        "\r\n",
        "Loading the data\r\n",
        "\r\n",
        "1. Loading the model and tokenizer\r\n",
        "2. Processing the data\r\n",
        "3. Fine-tuning RoBERTa\r\n",
        "4. Inference\r\n",
        "5. Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktYAvbEk5mAi"
      },
      "source": [
        "## 1. Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxMcmTpi5-pB"
      },
      "source": [
        "The data sets can be obtained using the Kaggle API. To do this, we must first upload our API token to the environment and put it in the appropriate folder. If you don't know how to use the API how generate a token, see [this link](https://github.com/Kaggle/kaggle-api)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "cArqSeP-6nqb",
        "outputId": "95eee0b5-fc0b-46b0-acb8-d7b4197702d3"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.upload();\r\n",
        "!mkdir /root/.kaggle\r\n",
        "!mv kaggle.json /root/.kaggle\r\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bd62fb4e-4335-4038-b2ef-ab6a41b7b73a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bd62fb4e-4335-4038-b2ef-ab6a41b7b73a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEbpm2rP7UBF"
      },
      "source": [
        "Now we can download the datasets for this competition with the following command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAqqkvgH6Fk-",
        "outputId": "b44ed9f8-3f08-45c2-c7b0-a09b54531068"
      },
      "source": [
        "!kaggle competitions download -c nlp-getting-started"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/22.2k [00:00<?, ?B/s]\n",
            "100% 22.2k/22.2k [00:00<00:00, 37.0MB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/411k [00:00<?, ?B/s]\n",
            "100% 411k/411k [00:00<00:00, 59.1MB/s]\n",
            "Downloading train.csv to /content\n",
            "  0% 0.00/965k [00:00<?, ?B/s]\n",
            "100% 965k/965k [00:00<00:00, 63.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BHEzu8y55ew"
      },
      "source": [
        "The files are given in CSV format so they can easily be loaded using pandas. The files contain several fields but we will only be interested in 'text' and 'target', containing the tweets and annotated classification, respectively. We place the labels in a numpy array and cast them as a float as this is the data type we will later use when we place this labels into a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGmrO2gC65E4"
      },
      "source": [
        "import random\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Load training and testing data\r\n",
        "df = pd.read_csv('train.csv',index_col=0)\r\n",
        "df_test = pd.read_csv('test.csv',index_col=0)\r\n",
        "# Extract 'text' and 'target' information from dataframe and shuffle the data\r\n",
        "temp = [(x,y) for x,y in zip(list(df['text']),list(df['target']))]\r\n",
        "random.shuffle(temp)\r\n",
        "tweets = [t[0] for t in temp]\r\n",
        "y = [t[1] for t in temp]\r\n",
        "# Cast the target labels as a float in preparation for passing it to Tensorflow\r\n",
        "y = np.array(y).astype('float32')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhP-Du4SW_w0"
      },
      "source": [
        "We can investigate the distribution of labels. There are slightly fewer positive than negative examples, however this does not represent a significant imbalance, so no further action is required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkLIVMJ_XF_q",
        "outputId": "1b484fa4-f9c1-406c-ed63-df343ee6080e"
      },
      "source": [
        "print('Observations in training set')\r\n",
        "print(df['target'].count())\r\n",
        "print()\r\n",
        "print('Label proportion in training set')\r\n",
        "print(df['target'].value_counts()/(sum(df['target'].value_counts())))\r\n",
        "print()\r\n",
        "print('Observations in test set')\r\n",
        "print(df_test['text'].count())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observations in training set\n",
            "7613\n",
            "\n",
            "Label proportion in training set\n",
            "0    0.57034\n",
            "1    0.42966\n",
            "Name: target, dtype: float64\n",
            "\n",
            "Observations in test set\n",
            "3263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbyO7S3iXIK3"
      },
      "source": [
        "## 2. Loading the model and tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuJpkn-GXMf9"
      },
      "source": [
        "The 'transformers' library contains many architectures useful for NLU. What makes this library particularly useful is that model checkpoints are available for a wide variety of models. This means that we don't have to train a new model from scratch but can instead load a pre-trained model and fine-tune it for whatever task we want. This is known as transfer learning and allows users to reuse previous knowledge, which is a more efficient way of advancing research.\r\n",
        "\r\n",
        "We download and install the transformers library running the following cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwLkMqGBXtf8",
        "outputId": "73c3cc7e-88de-4502-f9ea-c885f506f634"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5MB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 26.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 13.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=3675b39e47fe07e465b5d7bb8fd16dcee07619e76e32aa46b1d8b437c2a3a3d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBHJRr1oXrlH"
      },
      "source": [
        "Within RoBERTa, different implementations are available, including a model returning the last hidden states of RoBERTa as-is, and one with a classification head stacked on top which is useful for sentiment analysis and document classification. This notebook will make use of the version that is already prepared for classification as a Tensorflow model. For an analyses that make use of a transformer model as-is and stacks a customer-made classification head on top, see this approach (BERT).\r\n",
        "\r\n",
        "We also need to load the tokenizer that was used to originally train the model. The tokenizer includes the rules employed to tokenise text, the vocabulary and the dictionary mapping tokens to numerical indices. It is important we use exactly this tokenizer, as the model contains token representations that are identified by the token indices given by this tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4WkoTfTXPZV"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZxcxoewXU_R"
      },
      "source": [
        "BERT is available in two versions, BERT-base and BERT-large. The former counts with ~110M parameters and is a reduced version of the latter, which has ~340M parameters. Similarly, RoBERTa is also available in both versions. For the purpose of this competition, we use the large version. However, the difference in performance is relatively small, and the base version can still yield very good results. Moreover, one could also consider DistilBERT, a compact version of BERT with 40% less parameters than BERT-base. The difference in score in this competition when using DistilBERT-base compared to BERT-large was of about 0.02 points, however training and inference were much more faster. In real-life scenarios, it is important to consider the trade-off between performance and speed, and choose the appropriate model according to the requirements of the task.\r\n",
        "\r\n",
        "In the following cell, we instantiate the model and use 'from_pretrained' to specify that we want to load weights from an existing checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342,
          "referenced_widgets": [
            "fbf1716e0a844c5798771e9c13dd5fe9",
            "e21556df6128432cb8cccf77ffdce6c3",
            "bb16b9884a6f468a8d48cc52a43d89d0",
            "57a420cfc8af4156bcc123e45324c106",
            "ad5df795f1ca4dcbb621ea44f955e599",
            "abbba613a13c4e6aa6c94a3f9ab8864c",
            "7d959d96ef92485b9294eb8e7d37d84a",
            "fff1565559e84e4ba5e42621a3389b5b",
            "e7ac6bb0320a47afa4c32ed962fe1e02",
            "5098ebe9da6a4b60bfebec689880d6bf",
            "cd1737c191fd4fd7bcf97db95dca2da8",
            "a87d05db8d7d4b8da3d74a0c51b6b7bd",
            "b415473e25184809bdabbdf266f465b6",
            "1175aeeef525490784ed1e79a973ed9c",
            "52f6cd7563a945d39aa91171e2ece1af",
            "e2066e921c5b46b19939844213653e2a",
            "f8dd4ccb945943b49c3f5aa25df65e2c",
            "def658ed9aee4faba8b364307c2728c4",
            "0469f4abdb304502850830690513ac14",
            "532fc47b1b95448dbcfb1cfe1d59712d",
            "000aaca73e0e4d34b93fb198bc7f1149",
            "0d3624c491c14017941ef4a8a517a6c0",
            "1c9789f907554d29bb5a2378f4a8857b",
            "2fe0c6a2ad5d4db090d6a8d9f152a202",
            "221042f75b774ef7a96611759807ebce",
            "fadf7ffe467d4d70962bbb5704420850",
            "33cfe5c01d9e4cbcb0587c56828d6f5c",
            "b7b3e6e8f160443cb120b3d15c7f42c6",
            "18a0d494903a4bea8c9e0ecd033f8301",
            "1511354b81424002a7d6d6086208d991",
            "ecfa493d2e20446bb4b92d6ba6bef182",
            "60a84a117ace40e997f1c28f517f7b9b",
            "83ac1a7119d5474189cda0c916d64c08",
            "1d8c49dde70543eeb8e94db338addc8c",
            "33db587a832a4bbb9cfd5f7e9a5b9df4",
            "24131f99d0a743cbb88a9a63c9ac81d2",
            "674b87057a6842a1b71bdc7902c26050",
            "2df63f92ebfd453ebfc97980ad59ab57",
            "22a26991a9a249029f2588120f36d5fe",
            "c65719691c724fdd99694c33348cd345"
          ]
        },
        "id": "-SeLptbrX3c4",
        "outputId": "b2f46b7f-12a4-4d56-deec-5d1051a2a845"
      },
      "source": [
        "model_name = 'roberta-large'\r\n",
        "roberta_tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\r\n",
        "roberta_seq = TFRobertaForSequenceClassification.from_pretrained(model_name)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbf1716e0a844c5798771e9c13dd5fe9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7ac6bb0320a47afa4c32ed962fe1e02",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8dd4ccb945943b49c3f5aa25df65e2c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "221042f75b774ef7a96611759807ebce",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=482.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83ac1a7119d5474189cda0c916d64c08",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1634375628.0, style=ProgressStyle(descrâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tpmLp9KX55H"
      },
      "source": [
        "This instance contains the RoBERTa-large model with a classifier on top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1qryuubX86v",
        "outputId": "a534968a-d969-42bb-adef-507e5c560e1b"
      },
      "source": [
        "roberta_seq.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_roberta_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "roberta (TFRobertaMainLayer) multiple                  354310144 \n",
            "_________________________________________________________________\n",
            "classifier (TFRobertaClassif multiple                  1051650   \n",
            "=================================================================\n",
            "Total params: 355,361,794\n",
            "Trainable params: 355,361,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZcYsMzsX-rp"
      },
      "source": [
        "Once we have prepared the data sets in the next section, we will verify that the classifier consists of two units with no activation function, giving as output their bare activation values, also known as logits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvvAZZTcYAvj"
      },
      "source": [
        "## 3. Processing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp6ldRNgYCbd"
      },
      "source": [
        "When dealing with text data, some pre-processing may be necessary as data may not be of the same type that the model was trained on. Here we briefly consider only a few pre-processing steps.\r\n",
        "\r\n",
        "First, we know that tweets may contain tags (#tag) and mentions (@name). We will shortly see that the tokenizer can separate the special characters and read the tags and names as a word. It's not clear whether tags and names consisting of multiple words and special characters without space would repesent additional difficulties for the model. An option could be to remove these altogether. However, in this exercise, we keep tags and mentions as they appear.\r\n",
        "\r\n",
        "Next, note that the datasets do not contain unicode codes, which could stand for emojis, for example. This means this data already underwent some type of pre-processing before being published, as tweets often feature emojis in some form. In any case, we wont't have to worry about them in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DHuuOPxYE35"
      },
      "source": [
        "# Contains unicode codes (e.g. emojis)?\r\n",
        "for t in tweets:\r\n",
        "    if 'U+' in t:\r\n",
        "        print(t)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-__E1-KYHEp"
      },
      "source": [
        "However, the tweets contain some HTML character entities, these are text representations of special characters for HTML. For example: \"&gt\" is to be interpreted as \">\" (greater than). We can easily verify that this only occurs for 3 types of characters, \"&amp\", \"&gt\" and \"&lt\", corresponding to \"&\", \">\" and \"<\", respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkjnnW3wYI_J"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "# Contains HTML character entities?\r\n",
        "for t in tweets:\r\n",
        "    if '&' in re.sub(r'(&amp|&gt|&lt)','',t):\r\n",
        "        print(t)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIF6VnsLYK7z"
      },
      "source": [
        "Addionally, tweets may contain links, which are rendered in a standard format as \"http(s)://t.co/xxx\" where the (s) is optional and \"xxx\" stands for an alphanumeric string. Since we know that RoBERTa was trained on books, wikipedia articles and news articles, which do not feature links of this form, we could opt for removing these URLs.\r\n",
        "\r\n",
        "Other processing measures can also be considered. For instance, people often use slang, abbreviations and alternative spellings in their tweets, which are unlikely in the data set that RoBERTa was trained on. For instance, consider the common abbreviations in the following cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrQbwiJPYNNh",
        "outputId": "12acd694-8ba9-43dd-8011-feed03b4273c"
      },
      "source": [
        "for t in tweets:\r\n",
        "    if any([x in t for x in [' btw ',' omg ',' lol ',' thx ']]):\r\n",
        "        print(t)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Catching up on GBBO and omg that girls cake that just totally collapsed I feel so bad\n",
            "@allyinwondrland That sounds like the perfect bread! I'll hit up Trader Joes this wknd ??. Not really lol Already being inundated with\n",
            "can't DL a patch to fix the error in symantec as it's quarantined the computer ergo stopped all wireless exchange. lol technology ??\n",
            "Michael talking about when he was electrocuted omg #ROWYSOLouisville http://t.co/HxVfmoUhDM\n",
            "Best believe all the wrong decisions are being made out here in these Memphis streets during this here rainstorm lol my folk doe\n",
            "suddenly it's off &amp; on gloomy &amp; thunder so loud it shakes the windows? Not ever on the Bay Area. Miss me w/that lol http://t.co/x4eCGGvnSN\n",
            "@GuiltyGearXXACP yeah I know but blaze blue dont have a twitter lol I drew this a few weeks ago http://t.co/sk3l74FLzZ\n",
            "@Guy_Reginald lol more than welcome ??????\n",
            "@KerryKatona7 hello wud u kindly consider following me bak please I'm never any trouble lol many thanks :-)\n",
            "My portable closet has collapsed 3x and it finally broke and my mom said 'maybe u should get rid of some clothes' lol how about no\n",
            "I don't laugh out loud at many things. But man I really lol @ the big bang theory.\n",
            "@MzGracieBaby for the record im jumpin out the window early... i got @OfficialRealRap body bagging luck.. lol save the file\n",
            "I wanna tweet a 'niall thx for not making me was to electrocute myself' tweet but I'm scared I'll jinx it\n",
            "@editaxohaze then let the bagging body's begin lol ???? I ain't cuffed yet so it shouldn't be that bad!!\n",
            "I want to go back to Vegas for my 21 but I feel like that would be such a disaster lol so much money would need to be brought\n",
            "Apparently if you're bleeding people look at you weird lol well it's fine keep walking\n",
            "But if it's the apocalypse lol gf m8\n",
            "@Omar_molina036 @Milioooo_ he's trying to electrocute ya ass lol hell no I ain't fucking with Emilio no more ????????\n",
            "@atljw @cnnbrk fine line btw mass murderer and terrorist. Yes we don't know if there's polit. or social aspect yet; however he went to a\n",
            "Many thx for share and your comment Alex Lightman - \n",
            "\n",
            "What evidence did it take or will it take for you or your... http://t.co/4Wsva9WO0F\n",
            "@ScriptetteSar @katiecool447 btw the 30th is actually next year casualty began 6th September 1986 so 2016 marks 30 years\n",
            "@Cali74142290 lol natural disaster/hospital crisis something is needed to get rid of some cast members....\n",
            "@widda16 ... He's gone. You can relax. I thought the wife who wrecked her cake was a goner mind lol #whoops\n",
            "@TadhgTGMTEL dude was just smoking and the fucking thing went up in flames i though a bomb went off omg scared\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcbxbmuZYO_l"
      },
      "source": [
        "Although we could replace these expressions for their word equivalents (e.g. \"tbh\": \"to be honest\"), we can immediately see that there are only a handful of examples that contain these irregularities. Given that our training and testing files consist of thousands of examples, replacing these expressions will not have a large impact. In fact, these may be consider as adding some noise, which may help to prevent overfitting.\r\n",
        "\r\n",
        "Note that it is not immediately clear that we would benefit from more intrusive transformations, such as removing punctuation, numbers, undoing contractions or adding special tokens, because RoBERTa has been trained on text that contains all of these elements. Thus, the model should already be able to capture these basic elements of language as it has already seen them before. Misspelings are different, of course, however, we make no effort to fix them in this approach. You can consider running the tweets through a spelling checker and compare the results.\r\n",
        "\r\n",
        "In summary, we only perform two transformations to the data set, removing URLs and converting HTML character entities to their intended representation. If you prefer not to apply these transformations, simply comment out the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la0U25ZiYSUc"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "def process_tweets(tweets):\r\n",
        "    r = tweets\r\n",
        "    r = [re.sub(r'https?://t.co/\\w+','',t) for t in r]\r\n",
        "    r = [re.sub('&amp;','&',t) for t in r]\r\n",
        "    r = [re.sub('&gt;','gt',t) for t in r]\r\n",
        "    r = [re.sub('&lt;','lt',t) for t in r]\r\n",
        "    return r\r\n",
        "\r\n",
        "tweets = process_tweets(tweets)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK629JeGYT72"
      },
      "source": [
        "Before we tokenise the text, we should try to understand what the tokenizer does. As an experiment, we can call the tokenizer on the first 5 tweets in the data set. For illustrative purposes, we arbitrarily add a padding to obtain sequences of 50 tokens. The tokenizer returns an object which contains a dictionary with two elements: 'input_ids' and 'attention_mask'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW6K3LyRYV-q",
        "outputId": "99a02295-9488-4d9f-80ad-ce7c8f261126"
      },
      "source": [
        "temp = roberta_tokenizer(tweets[:5],padding='max_length',max_length=50)\r\n",
        "temp.keys()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rim7CpcEYXnK"
      },
      "source": [
        "'input_ids' are the indices assigned to each token. Decoding the 'input_ids' recovers the original tweet plus some special tokens that the tokenizer has introduced for the model. These special tokens include a start of sequence token \"< s >\" at the start of the document, an end of sequence token \"< \\s >\" at the end of a sequence and a padding token \"< pad >\" to fill a sequence to the maximum specified length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PyW9qKWYaEc",
        "outputId": "dd33b011-a88b-4851-91e5-036e549fc075"
      },
      "source": [
        "print('Original tweet:')\r\n",
        "print(tweets[0])\r\n",
        "print('Encoded tweet:')\r\n",
        "print(temp['input_ids'][0])\r\n",
        "print('Decoded tweet:')\r\n",
        "print(roberta_tokenizer.decode(temp['input_ids'][0]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original tweet:\n",
            " E-Mini SP 500: Earnings letdown equals market meltdown!  #Amazon\n",
            "Encoded tweet:\n",
            "[0, 381, 12, 44824, 6178, 1764, 35, 7535, 1033, 905, 3955, 27601, 210, 24053, 328, 1437, 849, 25146, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Decoded tweet:\n",
            "<s> E-Mini SP 500: Earnings letdown equals market meltdown!  #Amazon</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWTl-LxwYeir"
      },
      "source": [
        "The 'attention_mask' indicates whether a token in the encoded sequence corresponds to the \"< pad >\" token or not. It's function is to let the model know that these padding tokens are effectively blank spaces, so there is no need to pay attention to them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN6eFIvUYggF",
        "outputId": "7798dd35-bc78-4c87-94f2-5affffc453c0"
      },
      "source": [
        "print(temp['attention_mask'][0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khp2CDMKYh40"
      },
      "source": [
        "You can experiment by calling the tokenizer on more tweets to see how it treats numbers, tags (#), mentions (@), links and other elements present in the tweets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpTGntAvYjmH"
      },
      "source": [
        "Since the encoded tweets will be placed in a tensor for training and inference, they should all be of the same length, so we have to find the length of the longest encoded sequence and pad all tweets to that value. Note that it could be possible that the testing set contains a longer sequence than the training set. Thus, to make sure that we pad to the longest sequence we will find in this exercise, we combine training and testing sets, tokenise them together, and extract the maximum sequence length. In real-life applications where we don't know beforehand what is longest sequence we will find, we can add some arbitrary extra padding just to be safe.\r\n",
        "\r\n",
        "Important note: this is the only time we make use of this combined set, as training must be carried out only over the training set to prevent data leakage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFcaNEwqYl9F",
        "outputId": "a496f87b-b13c-4964-b44c-ff9a6833f1b1"
      },
      "source": [
        "# Combine all data in a separate list used only for determining the length of sequences\r\n",
        "all_tweets = list(pd.concat([df,df_test],axis=0)['text'])\r\n",
        "# Comment out this line if you don't apply any pre-processing\r\n",
        "all_tweets = process_tweets(all_tweets)\r\n",
        "max_len = max([len(t) for t in roberta_tokenizer(all_tweets)['input_ids']])\r\n",
        "print(max_len)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJt5-vgbYoCD"
      },
      "source": [
        "The available data is split into a training and a testing (or validation) set. Note that we ask the tokenizer to return Tensorflow tensors as that's the library we will be using here, however, one could also use PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXN7Uc8kYqaB"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets,y,test_size=0.30)\r\n",
        "X_train = roberta_tokenizer(X_train,padding='max_length',max_length=max_len,return_tensors='tf')\r\n",
        "X_test = roberta_tokenizer(X_test,padding='max_length',max_length=max_len,return_tensors='tf')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHoc3YWtYsZ6"
      },
      "source": [
        "We can verify that both sets contain a similar proportion of positive labels as the original set to make sure that the random splitting has not unintendedly introduced a class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogoxNYwgYtyb",
        "outputId": "5a376716-b101-4c51-a6cd-680983274740"
      },
      "source": [
        "print(y.sum()/len(y))\r\n",
        "print(y_train.sum()/len(y_train))\r\n",
        "print(y_test.sum()/len(y_test))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4296597924602653\n",
            "0.4270970163257647\n",
            "0.43563922942206657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go15FcSOYvJt"
      },
      "source": [
        "Next, the data is loaded into Tensorflow Datasets. These objects have built-in methods for shuffling and batching the data, and are more efficient for training and inference when dealing with large volumes of data. As RoBERTa-large is a rather heavy model, we have to choose a small batch size, otherwise the examples won't fit in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVPKcBJPYxS_"
      },
      "source": [
        "batch_size = 8\r\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train),y_train))\r\n",
        "train_dataset = train_dataset.batch(batch_size)\r\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(X_test),y_test))\r\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFqzHhdgYz3Z"
      },
      "source": [
        "Let's verify the structure of data sets. Each one contains a tuple, where the first element is a dictionary of the encoded tweets and the second is an array of labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udfHHDV8Y11S",
        "outputId": "2fbfe69f-3ec7-4e44-a3a6-3c77deab53fa"
      },
      "source": [
        "print(train_dataset)\r\n",
        "print(test_dataset)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ({input_ids: (None, 70), attention_mask: (None, 70)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.float32)>\n",
            "<BatchDataset shapes: ({input_ids: (None, 70), attention_mask: (None, 70)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjDdKiuGY3Qx"
      },
      "source": [
        "As a test to make sure we have the data in the right format, we can evaluate the model on the first batch of the training set and see what comes out. In the following cell, \"iter\" is used to cast the Dataset object as an iterator and \"next\" to take the first element, i.e. the first batch of 8 examples, which is spearated into inputs (temp_x) and labels (temp_y). We can verify that the model returns a pair of logits per example, as mentioned earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ7qsZXIY5Oo",
        "outputId": "d5876266-d3f5-4d90-964f-7f38e4589b5f"
      },
      "source": [
        "temp_x, temp_y = next(iter(test_dataset))\r\n",
        "temp = roberta_seq(temp_x,temp_y)\r\n",
        "temp"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFSequenceClassifierOutput([('logits',\n",
              "                             <tf.Tensor: shape=(8, 2), dtype=float32, numpy=\n",
              "                             array([[0.5460506 , 0.22686322],\n",
              "                                    [0.54130423, 0.30309516],\n",
              "                                    [0.524813  , 0.30203414],\n",
              "                                    [0.50775045, 0.30955476],\n",
              "                                    [0.5560991 , 0.26019868],\n",
              "                                    [0.512265  , 0.29737   ],\n",
              "                                    [0.5132245 , 0.30428982],\n",
              "                                    [0.53814274, 0.2567293 ]], dtype=float32)>)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4dW1rWqY60q"
      },
      "source": [
        "## 4. Fine-tuning RoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnMwADLuY9OG"
      },
      "source": [
        "\r\n",
        "\r\n",
        "Before we can fine-tune the model, we must add an optimiser and a loss function for training. We choose the 'adam' optimiser and set a small learning rate, as we are only doing a fine-tuning of the weights.\r\n",
        "\r\n",
        "As for the loss function, we choose cross entropy as this is a classification task. Since the last layer of this model contains 2 units, while our training targets (y) are given as a single value per example (i.e. indices, [0] or [1]), the function we must call from Tensorflow is SparseCategoricalCrossentropy. If our targets were given as two values per example (i.e. one-hot encoded, [0,1] or [1,0]), we would use CategoricalCrossentropy; and if the model had only one output, same as our targets, then we would use BinaryCrossentropy. Note that in this example (binary classification) these three functions are all equivalent, which one we choose depends only on the format of the data.\r\n",
        "\r\n",
        "In summary:\r\n",
        "\r\n",
        "* Model output: n elements. Target: 1 element. Use: SparseCategoricalCrossentropy\r\n",
        "* Model output: n elements. Target: n elements. Use: CategoricalCrossentropy\r\n",
        "* Model output: 1 element. Target: 1 element. Use: BinaryCrossentropy\r\n",
        "\r\n",
        "Furthermore, recall that the output of the last layer has no activation function, so the model is returning logits. Therefore, when we call the loss function from Tensorflow, we must pass the argument 'from_logits=True' to indicate that the outputs of the model should be passed through an activation function first when computing the loss. According to Tensorflow's [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy), this is more numerically stable than adding an activation function explicitly to the last layer of the model.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFUnD_xlZEYC"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6)\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "roberta_seq.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVVWh-O8ZSp6"
      },
      "source": [
        "Finally, we add a callback to save the checkpoints of the model and keep the one with the best performance only, measured by accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2l5BeEfZUre"
      },
      "source": [
        "chkpt = './checkpoint'\r\n",
        "callback_chkpt = tf.keras.callbacks.ModelCheckpoint(chkpt,\r\n",
        "                                              monitor='val_accuracy',\r\n",
        "                                              save_weights_only=True,\r\n",
        "                                              save_best_only=True,\r\n",
        "                                              mode='max')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF473AodZW5t"
      },
      "source": [
        "We can now train the model on our tweets dataset. The dataset objects are already batched, so there is no need to specify the batch size in here. It does not take too long to obtain a good validation score, so 2-3 epochs of training should be enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQkE-dUAZYvh",
        "outputId": "198e953b-e206-4def-d1c2-553f88d070b5"
      },
      "source": [
        "history = roberta_seq.fit(train_dataset,epochs=3,\r\n",
        "                          validation_data=test_dataset,\r\n",
        "                          callbacks=[callback_chkpt])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "667/667 [==============================] - ETA: 0s - loss: 0.5363 - accuracy: 0.7212"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r667/667 [==============================] - 349s 485ms/step - loss: 0.5362 - accuracy: 0.7213 - val_loss: 0.4226 - val_accuracy: 0.8297\n",
            "Epoch 2/3\n",
            "667/667 [==============================] - 323s 485ms/step - loss: 0.3570 - accuracy: 0.8542 - val_loss: 0.4353 - val_accuracy: 0.8257\n",
            "Epoch 3/3\n",
            "667/667 [==============================] - 329s 494ms/step - loss: 0.2977 - accuracy: 0.8852 - val_loss: 0.4690 - val_accuracy: 0.8288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TKRTNduZadX"
      },
      "source": [
        "Once training finishes, we restore the checkpoint with the best accuracy on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE3wk9jyZcyF",
        "outputId": "654a105f-6d03-4521-c2f4-e0d808a17d50"
      },
      "source": [
        "roberta_seq.load_weights(chkpt)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f019eaccc18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BPdvwwlZeth"
      },
      "source": [
        "Next, we can produce a classification report to compare the different metrics of the model. To do this, we compute and save the predictions on the labeled testing set. Calling 'predict' on the data outputs a tuple with a single element, an array of two logits. We asign a label based on the index of the largest value, which can be found using 'argmax' (we use 'axis=1' because the model returns a pair of logits for each example). Note that for inference, it is not necessary to apply an activation function to the logits, as this function does not change the result of taking 'argmax'. You can verify and convince yourself this is true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdphDXFrZiHt",
        "outputId": "47311aec-725a-4e0f-8b97-aebb994efa2c"
      },
      "source": [
        "outputs = roberta_seq.predict(test_dataset)\r\n",
        "y_pred = outputs[0].argmax(axis=1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y6tjYV3Zj2A"
      },
      "source": [
        "The confusion matrix and classification report are printed using sklearn's functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL2McSxGZlmX",
        "outputId": "da7f839d-1a1a-434b-c326-4db5730b0be1"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\r\n",
        "\r\n",
        "print('Confusion matrix:')\r\n",
        "print(confusion_matrix(y_test,y_pred,labels=[0,1]))\r\n",
        "print()\r\n",
        "print('Classification report:')\r\n",
        "print(classification_report(y_test,y_pred,labels=[0,1],target_names=['not a disaster','disaster']))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n",
            "[[1122  167]\n",
            " [ 222  773]]\n",
            "\n",
            "Classification report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "not a disaster       0.83      0.87      0.85      1289\n",
            "      disaster       0.82      0.78      0.80       995\n",
            "\n",
            "      accuracy                           0.83      2284\n",
            "     macro avg       0.83      0.82      0.83      2284\n",
            "  weighted avg       0.83      0.83      0.83      2284\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mffK9rQZoh3"
      },
      "source": [
        "There is an element of randomness arising from how the data is split intro training and evaluation sets, and in the training process. This can lead to small variations in the performance of the model. Over several iterations of loading and fine-tuning the model, I have obtained an F1-score of 0.83-0.85 on the evaluation set, and 0.8274-0.8424 on the leaderboard of the competition (the real test set). Thus, the impact of these random factors on the score is rather small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rFw_hx5ZpJo"
      },
      "source": [
        "## 5. Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZAVITIQZsGh"
      },
      "source": [
        "We are now ready to make predictions on the real test set. We extract the tweets from the test file, pass them through the tokeniser and place them into a batched dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmdjofJJZt7Z",
        "outputId": "53bc6789-d1ed-48a6-f471-f8345a817ed3"
      },
      "source": [
        "tweets_test = list(df_test['text'])\r\n",
        "# Comment out this line if you are not doing pre-processing\r\n",
        "tweets_test = process_tweets(tweets_test)\r\n",
        "X_real_test = roberta_tokenizer(tweets_test,padding='max_length',max_length=max_len,return_tensors='tf')\r\n",
        "real_test_dataset = tf.data.Dataset.from_tensor_slices(dict(X_real_test))\r\n",
        "real_test_dataset = real_test_dataset.batch(batch_size)\r\n",
        "real_test_dataset"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: {input_ids: (None, 70), attention_mask: (None, 70)}, types: {input_ids: tf.int32, attention_mask: tf.int32}>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4JNSjhuZv64"
      },
      "source": [
        "The labels are assigned the same way as before, taking the 'argmax' from the outputs of the model for each observation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMz7KhlXZxqR",
        "outputId": "4dfc599d-ae36-4b19-d4da-6e11a4c79b2a"
      },
      "source": [
        "outputs_test = roberta_seq.predict(real_test_dataset)\r\n",
        "y_pred_test = outputs_test[0].argmax(axis=1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwF2L7rzZzbw"
      },
      "source": [
        "Finally, we can check the proportion of the predicted positive cases in the test set. Assuming that the observations in the test and training sets come from the same distribution and are randomly sampled, the proportion of positive cases should be similar to what we saw before, ~43%. This does not say anything about how good the model is, but if there is a large difference, it can indicate that something is going wrong, either with the model, or with the way the data is distributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwrWazz1Z1bR",
        "outputId": "da9f2a44-c22b-4cb3-f72d-2f49358f5280"
      },
      "source": [
        "y_pred_test.sum()/len(y_pred_test)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4008581060373889"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYoE50yWZ3Ab"
      },
      "source": [
        "Assign the id to each prediction and export the data as a csv file ready for submission to the Kaggle competiton."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYNyRJXVZ4k7"
      },
      "source": [
        "results = pd.Series(y_pred_test,index=df_test.index,name='target')\r\n",
        "results.to_csv('./submission.csv')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ3-O0rfZ6Uc"
      },
      "source": [
        "## 6. Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKJV8TiKZ8--"
      },
      "source": [
        "This notebook has illustrated how to use the ðŸ¤— Huggingface's transformers library to solve a classification task applied to tweets. The RoBERTa-large model with a classifier layer on top is easy to use and can be fine-tuned in just a few epochs. Even with very limited pre-processing of the text, the model achieves a good F1-score showing that it can classify tweets correctly most of the time. Smaller models are also available which sacrifice only a little performance for a great boost in training and inference speed."
      ]
    }
  ]
}